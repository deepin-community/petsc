<center><a href="https://gitlab.com/petsc/petsc/-/blob/c7d19d7b3f2d9e6411c648820d1207320e4154c7/src/sys/utils/mpishm.c">Actual source code: mpishm.c</a></center><br>

<html>
<head>
<title></title>
<meta name="generator" content="c2html 0.9.4">
<meta name="date" content="2023-03-30T15:47:02+00:00">
</head>

<body bgcolor="#FFFFFF">
<pre width="80">
<a name="line1">  1: </a>#include <A href="../../../include/petscsys.h.html">&lt;petscsys.h&gt;</A>
<a name="line2">  2: </a>#include <A href="../../../include/petsc/private/petscimpl.h.html">&lt;petsc/private/petscimpl.h&gt;</A>

<a name="line4">  4: </a><font color="#4169E1"><a name="_n_PetscShmComm"></a>struct _n_PetscShmComm </font>{
<a name="line5">  5: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> *globranks;         <font color="#B22222">/* global ranks of each rank in the shared memory communicator */</font>
<a name="line6">  6: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a>  shmsize;           <font color="#B22222">/* size of the shared memory communicator */</font>
<a name="line7">  7: </a>  <a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a>     globcomm, shmcomm; <font color="#B22222">/* global communicator and shared memory communicator (a sub-communicator of the former) */</font>
<a name="line8">  8: </a>};

<a name="line10"> 10: </a><font color="#B22222">/*</font>
<a name="line11"> 11: </a><font color="#B22222">   Private routine to delete internal shared memory communicator when a communicator is freed.</font>

<a name="line13"> 13: </a><font color="#B22222">   This is called by MPI, not by users. This is called by <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_free.html#MPI_Comm_free">MPI_Comm_free</a>() when the communicator that has this  data as an attribute is freed.</font>

<a name="line15"> 15: </a><font color="#B22222">   Note: this is declared extern "C" because it is passed to <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_create_keyval.html#MPI_Comm_create_keyval">MPI_Comm_create_keyval</a>()</font>

<a name="line17"> 17: </a><font color="#B22222">*/</font>
<a name="line18"> 18: </a><strong><font color="#4169E1"><a name="Petsc_ShmComm_Attr_Delete_Fn"></a>PETSC_EXTERN <a href="../../../docs/manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> MPIAPI Petsc_ShmComm_Attr_Delete_Fn(<a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a> comm, <a href="../../../docs/manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> keyval, void *val, void *extra_state)</font></strong>
<a name="line19"> 19: </a>{
<a name="line20"> 20: </a>  PetscShmComm p = (PetscShmComm)val;

<a name="line22"> 22: </a>  <a href="../../../docs/manualpages/Profiling/PetscInfo.html">PetscInfo</a>(NULL, <font color="#666666">"Deleting shared memory subcommunicator in a <a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a> %ld\n"</font>, (long)comm);
<a name="line23"> 23: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_free.html#MPI_Comm_free">MPI_Comm_free</a>(&amp;p-&gt;shmcomm);
<a name="line24"> 24: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html">PetscFree</a>(p-&gt;globranks);
<a name="line25"> 25: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html">PetscFree</a>(val);
<a name="line26"> 26: </a>  <font color="#4169E1">return</font> MPI_SUCCESS;
<a name="line27"> 27: </a>}

<a name="line29"> 29: </a><font color="#A020F0">#ifdef PETSC_HAVE_MPI_PROCESS_SHARED_MEMORY</font>
<a name="line30"> 30: </a>  <font color="#B22222">/* Data structures to support freeing comms created in <a href="../../../docs/manualpages/Sys/PetscShmCommGet.html">PetscShmCommGet</a>().</font>
<a name="line31"> 31: </a><font color="#B22222">  Since we predict communicators passed to <a href="../../../docs/manualpages/Sys/PetscShmCommGet.html">PetscShmCommGet</a>() are very likely</font>
<a name="line32"> 32: </a><font color="#B22222">  either a petsc inner communicator or an MPI communicator with a linked petsc</font>
<a name="line33"> 33: </a><font color="#B22222">  inner communicator, we use a simple static array to store dupped communicators</font>
<a name="line34"> 34: </a><font color="#B22222">  on rare cases otherwise.</font>
<a name="line35"> 35: </a><font color="#B22222"> */</font>
<a name="line36"> 36: </a><strong><font color="#228B22">  #define MAX_SHMCOMM_DUPPED_COMMS 16</font></strong>
<a name="line37"> 37: </a>static <a href="../../../docs/manualpages/Sys/PetscInt.html">PetscInt</a>       num_dupped_comms = 0;
<a name="line38"> 38: </a>static <a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a>       shmcomm_dupped_comms[MAX_SHMCOMM_DUPPED_COMMS];
<a name="line39"> 39: </a><strong><font color="#4169E1"><a name="PetscShmCommDestroyDuppedComms"></a>static <a href="../../../docs/manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscShmCommDestroyDuppedComms(void)</font></strong>
<a name="line40"> 40: </a>{
<a name="line41"> 41: </a>  <a href="../../../docs/manualpages/Sys/PetscInt.html">PetscInt</a> i;
<a name="line42"> 42: </a>  <font color="#4169E1">for</font> (i = 0; i &lt; num_dupped_comms; i++) <a href="../../../docs/manualpages/Sys/PetscCommDestroy.html">PetscCommDestroy</a>(&amp;shmcomm_dupped_comms[i]);
<a name="line43"> 43: </a>  num_dupped_comms = 0; <font color="#B22222">/* reset so that PETSc can be reinitialized */</font>
<a name="line44"> 44: </a>  <font color="#4169E1">return</font> 0;
<a name="line45"> 45: </a>}
<a name="line46"> 46: </a><font color="#A020F0">#endif</font>

<a name="line48"> 48: </a><font color="#B22222">/*@C</font>
<a name="line49"> 49: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscShmCommGet.html">PetscShmCommGet</a> - Returns a sub-communicator of all ranks that share a common memory</font>

<a name="line51"> 51: </a><font color="#B22222">    Collective.</font>

<a name="line53"> 53: </a><font color="#B22222">    Input Parameter:</font>
<a name="line54"> 54: </a><font color="#B22222">.   globcomm - `<a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a>`, which can be a user <a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a> or a PETSc inner <a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a></font>

<a name="line56"> 56: </a><font color="#B22222">    Output Parameter:</font>
<a name="line57"> 57: </a><font color="#B22222">.   pshmcomm - the PETSc shared memory communicator object</font>

<a name="line59"> 59: </a><font color="#B22222">    Level: developer</font>

<a name="line61"> 61: </a><font color="#B22222">    Note:</font>
<a name="line62"> 62: </a><font color="#B22222">       When used with MPICH, MPICH must be configured with --download-mpich-device=ch3:nemesis</font>

<a name="line64"> 64: </a><font color="#B22222">.seealso: `<a href="../../../docs/manualpages/Sys/PetscShmCommGlobalToLocal.html">PetscShmCommGlobalToLocal</a>()`, `<a href="../../../docs/manualpages/Sys/PetscShmCommLocalToGlobal.html">PetscShmCommLocalToGlobal</a>()`, `<a href="../../../docs/manualpages/Sys/PetscShmCommGetMpiShmComm.html">PetscShmCommGetMpiShmComm</a>()`</font>
<a name="line65"> 65: </a><font color="#B22222">@*/</font>
<a name="line66"> 66: </a><strong><font color="#4169E1"><a name="PetscShmCommGet"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscShmCommGet.html">PetscShmCommGet</a>(<a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a> globcomm, PetscShmComm *pshmcomm)</font></strong>
<a name="line67"> 67: </a>{
<a name="line68"> 68: </a><font color="#A020F0">#ifdef PETSC_HAVE_MPI_PROCESS_SHARED_MEMORY</font>
<a name="line69"> 69: </a>  MPI_Group         globgroup, shmgroup;
<a name="line70"> 70: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a>      *shmranks, i, flg;
<a name="line71"> 71: </a>  PetscCommCounter *counter;

<a name="line74"> 74: </a>  <font color="#B22222">/* Get a petsc inner comm, since we always want to stash pshmcomm on petsc inner comms */</font>
<a name="line75"> 75: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_get_attr.html#MPI_Comm_get_attr">MPI_Comm_get_attr</a>(globcomm, Petsc_Counter_keyval, &amp;counter, &amp;flg);
<a name="line76"> 76: </a>  <font color="#4169E1">if</font> (!flg) { <font color="#B22222">/* globcomm is not a petsc comm */</font>
<a name="line77"> 77: </a>    <font color="#4169E1">union</font>
<a name="line78"> 78: </a>    {
<a name="line79"> 79: </a>      <a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a> comm;
<a name="line80"> 80: </a>      void    *ptr;
<a name="line81"> 81: </a>    } ucomm;
<a name="line82"> 82: </a>    <font color="#B22222">/* check if globcomm already has a linked petsc inner comm */</font>
<a name="line83"> 83: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_get_attr.html#MPI_Comm_get_attr">MPI_Comm_get_attr</a>(globcomm, Petsc_InnerComm_keyval, &amp;ucomm, &amp;flg);
<a name="line84"> 84: </a>    <font color="#4169E1">if</font> (!flg) {
<a name="line85"> 85: </a>      <font color="#B22222">/* globcomm does not have a linked petsc inner comm, so we create one and replace globcomm with it */</font>
<a name="line87"> 87: </a>      <a href="../../../docs/manualpages/Sys/PetscCommDuplicate.html">PetscCommDuplicate</a>(globcomm, &amp;globcomm, NULL);
<a name="line88"> 88: </a>      <font color="#B22222">/* Register a function to free the dupped petsc comms at <a href="../../../docs/manualpages/Sys/PetscFinalize.html">PetscFinalize</a> at the first time */</font>
<a name="line89"> 89: </a>      <font color="#4169E1">if</font> (num_dupped_comms == 0) <a href="../../../docs/manualpages/Sys/PetscRegisterFinalize.html">PetscRegisterFinalize</a>(PetscShmCommDestroyDuppedComms);
<a name="line90"> 90: </a>      shmcomm_dupped_comms[num_dupped_comms] = globcomm;
<a name="line91"> 91: </a>      num_dupped_comms++;
<a name="line92"> 92: </a>    } <font color="#4169E1">else</font> {
<a name="line93"> 93: </a>      <font color="#B22222">/* otherwise, we pull out the inner comm and use it as globcomm */</font>
<a name="line94"> 94: </a>      globcomm = ucomm.comm;
<a name="line95"> 95: </a>    }
<a name="line96"> 96: </a>  }

<a name="line98"> 98: </a>  <font color="#B22222">/* Check if globcomm already has an attached pshmcomm. If no, create one */</font>
<a name="line99"> 99: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_get_attr.html#MPI_Comm_get_attr">MPI_Comm_get_attr</a>(globcomm, Petsc_ShmComm_keyval, pshmcomm, &amp;flg);
<a name="line100">100: </a>  <font color="#4169E1">if</font> (flg) <font color="#4169E1">return</font> 0;

<a name="line102">102: </a>  <a href="../../../docs/manualpages/Sys/PetscNew.html">PetscNew</a>(pshmcomm);
<a name="line103">103: </a>  (*pshmcomm)-&gt;globcomm = globcomm;

<a name="line105">105: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_split_type.html#MPI_Comm_split_type">MPI_Comm_split_type</a>(globcomm, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &amp;(*pshmcomm)-&gt;shmcomm);

<a name="line107">107: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_size.html#MPI_Comm_size">MPI_Comm_size</a>((*pshmcomm)-&gt;shmcomm, &amp;(*pshmcomm)-&gt;shmsize);
<a name="line108">108: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_group.html#MPI_Comm_group">MPI_Comm_group</a>(globcomm, &amp;globgroup);
<a name="line109">109: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_group.html#MPI_Comm_group">MPI_Comm_group</a>((*pshmcomm)-&gt;shmcomm, &amp;shmgroup);
<a name="line110">110: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html">PetscMalloc1</a>((*pshmcomm)-&gt;shmsize, &amp;shmranks);
<a name="line111">111: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html">PetscMalloc1</a>((*pshmcomm)-&gt;shmsize, &amp;(*pshmcomm)-&gt;globranks);
<a name="line112">112: </a>  <font color="#4169E1">for</font> (i = 0; i &lt; (*pshmcomm)-&gt;shmsize; i++) shmranks[i] = i;
<a name="line113">113: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Group_translate_ranks.html#MPI_Group_translate_ranks">MPI_Group_translate_ranks</a>(shmgroup, (*pshmcomm)-&gt;shmsize, shmranks, globgroup, (*pshmcomm)-&gt;globranks);
<a name="line114">114: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html">PetscFree</a>(shmranks);
<a name="line115">115: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Group_free.html#MPI_Group_free">MPI_Group_free</a>(&amp;globgroup);
<a name="line116">116: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Group_free.html#MPI_Group_free">MPI_Group_free</a>(&amp;shmgroup);

<a name="line118">118: </a>  <font color="#4169E1">for</font> (i = 0; i &lt; (*pshmcomm)-&gt;shmsize; i++) <a href="../../../docs/manualpages/Profiling/PetscInfo.html">PetscInfo</a>(NULL, <font color="#666666">"Shared memory rank %d global rank %d\n"</font>, i, (*pshmcomm)-&gt;globranks[i]);
<a name="line119">119: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_set_attr.html#MPI_Comm_set_attr">MPI_Comm_set_attr</a>(globcomm, Petsc_ShmComm_keyval, *pshmcomm);
<a name="line120">120: </a>  <font color="#4169E1">return</font> 0;
<a name="line121">121: </a><font color="#A020F0">#else</font>
<a name="line122">122: </a>  <a href="../../../docs/manualpages/Sys/SETERRQ.html">SETERRQ</a>(globcomm, PETSC_ERR_SUP, <font color="#666666">"Shared memory communicators need MPI-3 package support.\nPlease upgrade your MPI or reconfigure with --download-mpich."</font>);
<a name="line123">123: </a><font color="#A020F0">#endif</font>
<a name="line124">124: </a>}

<a name="line126">126: </a><font color="#B22222">/*@C</font>
<a name="line127">127: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscShmCommGlobalToLocal.html">PetscShmCommGlobalToLocal</a> - Given a global rank returns the local rank in the shared memory communicator</font>

<a name="line129">129: </a><font color="#B22222">    Input Parameters:</font>
<a name="line130">130: </a><font color="#B22222">+   pshmcomm - the shared memory communicator object</font>
<a name="line131">131: </a><font color="#B22222">-   grank    - the global rank</font>

<a name="line133">133: </a><font color="#B22222">    Output Parameter:</font>
<a name="line134">134: </a><font color="#B22222">.   lrank - the local rank, or `MPI_PROC_NULL` if it does not exist</font>

<a name="line136">136: </a><font color="#B22222">    Level: developer</font>

<a name="line138">138: </a><font color="#B22222">    Developer Notes:</font>
<a name="line139">139: </a><font color="#B22222">    Assumes the pshmcomm-&gt;globranks[] is sorted</font>

<a name="line141">141: </a><font color="#B22222">    It may be better to rewrite this to map multiple global ranks to local in the same function call</font>

<a name="line143">143: </a><font color="#B22222">.seealso: `<a href="../../../docs/manualpages/Sys/PetscShmCommGet.html">PetscShmCommGet</a>()`, `<a href="../../../docs/manualpages/Sys/PetscShmCommLocalToGlobal.html">PetscShmCommLocalToGlobal</a>()`, `<a href="../../../docs/manualpages/Sys/PetscShmCommGetMpiShmComm.html">PetscShmCommGetMpiShmComm</a>()`</font>
<a name="line144">144: </a><font color="#B22222">@*/</font>
<a name="line145">145: </a><strong><font color="#4169E1"><a name="PetscShmCommGlobalToLocal"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscShmCommGlobalToLocal.html">PetscShmCommGlobalToLocal</a>(PetscShmComm pshmcomm, <a href="../../../docs/manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> grank, <a href="../../../docs/manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> *lrank)</font></strong>
<a name="line146">146: </a>{
<a name="line147">147: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> low, high, t, i;
<a name="line148">148: </a>  <a href="../../../docs/manualpages/Sys/PetscBool.html">PetscBool</a>   flg = <a href="../../../docs/manualpages/Sys/PETSC_FALSE.html">PETSC_FALSE</a>;

<a name="line152">152: </a>  *lrank = MPI_PROC_NULL;
<a name="line153">153: </a>  <font color="#4169E1">if</font> (grank &lt; pshmcomm-&gt;globranks[0]) <font color="#4169E1">return</font> 0;
<a name="line154">154: </a>  <font color="#4169E1">if</font> (grank &gt; pshmcomm-&gt;globranks[pshmcomm-&gt;shmsize - 1]) <font color="#4169E1">return</font> 0;
<a name="line155">155: </a>  <a href="../../../docs/manualpages/Sys/PetscOptionsGetBool.html">PetscOptionsGetBool</a>(NULL, NULL, <font color="#666666">"-noshared"</font>, &amp;flg, NULL);
<a name="line156">156: </a>  <font color="#4169E1">if</font> (flg) <font color="#4169E1">return</font> 0;
<a name="line157">157: </a>  low  = 0;
<a name="line158">158: </a>  high = pshmcomm-&gt;shmsize;
<a name="line159">159: </a>  <font color="#4169E1">while</font> (high - low &gt; 5) {
<a name="line160">160: </a>    t = (low + high) / 2;
<a name="line161">161: </a>    <font color="#4169E1">if</font> (pshmcomm-&gt;globranks[t] &gt; grank) high = t;
<a name="line162">162: </a>    <font color="#4169E1">else</font> low = t;
<a name="line163">163: </a>  }
<a name="line164">164: </a>  <font color="#4169E1">for</font> (i = low; i &lt; high; i++) {
<a name="line165">165: </a>    <font color="#4169E1">if</font> (pshmcomm-&gt;globranks[i] &gt; grank) <font color="#4169E1">return</font> 0;
<a name="line166">166: </a>    <font color="#4169E1">if</font> (pshmcomm-&gt;globranks[i] == grank) {
<a name="line167">167: </a>      *lrank = i;
<a name="line168">168: </a>      <font color="#4169E1">return</font> 0;
<a name="line169">169: </a>    }
<a name="line170">170: </a>  }
<a name="line171">171: </a>  <font color="#4169E1">return</font> 0;
<a name="line172">172: </a>}

<a name="line174">174: </a><font color="#B22222">/*@C</font>
<a name="line175">175: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscShmCommLocalToGlobal.html">PetscShmCommLocalToGlobal</a> - Given a local rank in the shared memory communicator returns the global rank</font>

<a name="line177">177: </a><font color="#B22222">    Input Parameters:</font>
<a name="line178">178: </a><font color="#B22222">+   pshmcomm - the shared memory communicator object</font>
<a name="line179">179: </a><font color="#B22222">-   lrank    - the local rank in the shared memory communicator</font>

<a name="line181">181: </a><font color="#B22222">    Output Parameter:</font>
<a name="line182">182: </a><font color="#B22222">.   grank - the global rank in the global communicator where the shared memory communicator is built</font>

<a name="line184">184: </a><font color="#B22222">    Level: developer</font>

<a name="line186">186: </a><font color="#B22222">.seealso: `<a href="../../../docs/manualpages/Sys/PetscShmCommGlobalToLocal.html">PetscShmCommGlobalToLocal</a>()`, `<a href="../../../docs/manualpages/Sys/PetscShmCommGet.html">PetscShmCommGet</a>()`, `<a href="../../../docs/manualpages/Sys/PetscShmCommGetMpiShmComm.html">PetscShmCommGetMpiShmComm</a>()`</font>
<a name="line187">187: </a><font color="#B22222">@*/</font>
<a name="line188">188: </a><strong><font color="#4169E1"><a name="PetscShmCommLocalToGlobal"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscShmCommLocalToGlobal.html">PetscShmCommLocalToGlobal</a>(PetscShmComm pshmcomm, <a href="../../../docs/manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> lrank, <a href="../../../docs/manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> *grank)</font></strong>
<a name="line189">189: </a>{
<a name="line193">193: </a>  *grank = pshmcomm-&gt;globranks[lrank];
<a name="line194">194: </a>  <font color="#4169E1">return</font> 0;
<a name="line195">195: </a>}

<a name="line197">197: </a><font color="#B22222">/*@C</font>
<a name="line198">198: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscShmCommGetMpiShmComm.html">PetscShmCommGetMpiShmComm</a> - Returns the MPI communicator that represents all processes with common shared memory</font>

<a name="line200">200: </a><font color="#B22222">    Input Parameter:</font>
<a name="line201">201: </a><font color="#B22222">.   pshmcomm - PetscShmComm object obtained with <a href="../../../docs/manualpages/Sys/PetscShmCommGet.html">PetscShmCommGet</a>()</font>

<a name="line203">203: </a><font color="#B22222">    Output Parameter:</font>
<a name="line204">204: </a><font color="#B22222">.   comm     - the MPI communicator</font>

<a name="line206">206: </a><font color="#B22222">    Level: developer</font>

<a name="line208">208: </a><font color="#B22222">.seealso: `<a href="../../../docs/manualpages/Sys/PetscShmCommGlobalToLocal.html">PetscShmCommGlobalToLocal</a>()`, `<a href="../../../docs/manualpages/Sys/PetscShmCommGet.html">PetscShmCommGet</a>()`, `<a href="../../../docs/manualpages/Sys/PetscShmCommLocalToGlobal.html">PetscShmCommLocalToGlobal</a>()`</font>
<a name="line209">209: </a><font color="#B22222">@*/</font>
<a name="line210">210: </a><strong><font color="#4169E1"><a name="PetscShmCommGetMpiShmComm"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscShmCommGetMpiShmComm.html">PetscShmCommGetMpiShmComm</a>(PetscShmComm pshmcomm, <a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a> *comm)</font></strong>
<a name="line211">211: </a>{
<a name="line214">214: </a>  *comm = pshmcomm-&gt;shmcomm;
<a name="line215">215: </a>  <font color="#4169E1">return</font> 0;
<a name="line216">216: </a>}

<a name="line218">218: </a><font color="#A020F0">#if defined(PETSC_HAVE_OPENMP_SUPPORT)</font>
<a name="line219">219: </a><font color="#A020F0">  #include &lt;pthread.h&gt;</font>
<a name="line220">220: </a><font color="#A020F0">  #include &lt;hwloc.h&gt;</font>
<a name="line221">221: </a><font color="#A020F0">  #include &lt;omp.h&gt;</font>

<a name="line223">223: </a>  <font color="#B22222">/* Use mmap() to allocate shared mmeory (for the pthread_barrier_t object) if it is available,</font>
<a name="line224">224: </a><font color="#B22222">   otherwise use <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Win_allocate_shared.html#MPI_Win_allocate_shared">MPI_Win_allocate_shared</a>. They should have the same effect except MPI-3 is much</font>
<a name="line225">225: </a><font color="#B22222">   simpler to use. However, on a Cori Haswell node with Cray MPI, MPI-3 worsened a test's performance</font>
<a name="line226">226: </a><font color="#B22222">   by 50%. Until the reason is found out, we use mmap() instead.</font>
<a name="line227">227: </a><font color="#B22222">*/</font>
<a name="line228">228: </a><strong><font color="#228B22">  #define USE_MMAP_ALLOCATE_SHARED_MEMORY</font></strong>

<a name="line230">230: </a><font color="#A020F0">  #if defined(USE_MMAP_ALLOCATE_SHARED_MEMORY) &amp;&amp; defined(PETSC_HAVE_MMAP)</font>
<a name="line231">231: </a><font color="#A020F0">    #include &lt;sys/mman.h&gt;</font>
<a name="line232">232: </a><font color="#A020F0">    #include &lt;sys/types.h&gt;</font>
<a name="line233">233: </a><font color="#A020F0">    #include &lt;sys/stat.h&gt;</font>
<a name="line234">234: </a><font color="#A020F0">    #include &lt;fcntl.h&gt;</font>
<a name="line235">235: </a><font color="#A020F0">  #endif</font>

<a name="line237">237: </a><font color="#4169E1"><a name="_n_PetscOmpCtrl"></a>struct _n_PetscOmpCtrl </font>{
<a name="line238">238: </a>  <a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a>           omp_comm;        <font color="#B22222">/* a shared memory communicator to spawn omp threads */</font>
<a name="line239">239: </a>  <a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a>           omp_master_comm; <font color="#B22222">/* a communicator to give to third party libraries */</font>
<a name="line240">240: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a>        omp_comm_size;   <font color="#B22222">/* size of omp_comm, a kind of OMP_NUM_THREADS */</font>
<a name="line241">241: </a>  <a href="../../../docs/manualpages/Sys/PetscBool.html">PetscBool</a>          is_omp_master;   <font color="#B22222">/* rank 0's in omp_comm */</font>
<a name="line242">242: </a>  MPI_Win            omp_win;         <font color="#B22222">/* a shared memory window containing a barrier */</font>
<a name="line243">243: </a>  pthread_barrier_t *barrier;         <font color="#B22222">/* pointer to the barrier */</font>
<a name="line244">244: </a>  hwloc_topology_t   topology;
<a name="line245">245: </a>  hwloc_cpuset_t     cpuset;     <font color="#B22222">/* cpu bindings of omp master */</font>
<a name="line246">246: </a>  hwloc_cpuset_t     omp_cpuset; <font color="#B22222">/* union of cpu bindings of ranks in omp_comm */</font>
<a name="line247">247: </a>};

<a name="line249">249: </a><font color="#B22222">/* Allocate and initialize a pthread_barrier_t object in memory shared by processes in omp_comm</font>
<a name="line250">250: </a><font color="#B22222">   contained by the controller.</font>

<a name="line252">252: </a><font color="#B22222">   PETSc OpenMP controller users do not call this function directly. This function exists</font>
<a name="line253">253: </a><font color="#B22222">   only because we want to separate shared memory allocation methods from other code.</font>
<a name="line254">254: </a><font color="#B22222"> */</font>
<a name="line255">255: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlCreateBarrier"></a>static inline <a href="../../../docs/manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscOmpCtrlCreateBarrier(PetscOmpCtrl ctrl)</font></strong>
<a name="line256">256: </a>{
<a name="line257">257: </a>  MPI_Aint              size;
<a name="line258">258: </a>  void                 *baseptr;
<a name="line259">259: </a>  pthread_barrierattr_t attr;

<a name="line261">261: </a><font color="#A020F0">  #if defined(USE_MMAP_ALLOCATE_SHARED_MEMORY) &amp;&amp; defined(PETSC_HAVE_MMAP)</font>
<a name="line262">262: </a>  <a href="../../../docs/manualpages/Sys/PetscInt.html">PetscInt</a>  fd;
<a name="line263">263: </a>  PetscChar pathname[PETSC_MAX_PATH_LEN];
<a name="line264">264: </a><font color="#A020F0">  #else</font>
<a name="line265">265: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a> disp_unit;
<a name="line266">266: </a><font color="#A020F0">  #endif</font>

<a name="line268">268: </a><font color="#A020F0">  #if defined(USE_MMAP_ALLOCATE_SHARED_MEMORY) &amp;&amp; defined(PETSC_HAVE_MMAP)</font>
<a name="line269">269: </a>  size = <font color="#4169E1">sizeof</font>(pthread_barrier_t);
<a name="line270">270: </a>  <font color="#4169E1">if</font> (ctrl-&gt;is_omp_master) {
<a name="line271">271: </a>    <font color="#B22222">/* use <a href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html">PETSC_COMM_SELF</a> in <a href="../../../docs/manualpages/Sys/PetscGetTmp.html">PetscGetTmp</a>, since it is a collective call. Using omp_comm would otherwise bcast the partially populated pathname to slaves */</font>
<a name="line272">272: </a>    <a href="../../../docs/manualpages/Sys/PetscGetTmp.html">PetscGetTmp</a>(<a href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html">PETSC_COMM_SELF</a>, pathname, PETSC_MAX_PATH_LEN);
<a name="line273">273: </a>    <a href="../../../docs/manualpages/Sys/PetscStrlcat.html">PetscStrlcat</a>(pathname, <font color="#666666">"/petsc-shm-XXXXXX"</font>, PETSC_MAX_PATH_LEN);
<a name="line274">274: </a>    <font color="#B22222">/* mkstemp replaces XXXXXX with a unique file name and opens the file for us */</font>
<a name="line275">275: </a>    fd = mkstemp(pathname);
<a name="line277">277: </a>    ftruncate(fd, size);
<a name="line278">278: </a>    baseptr = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
<a name="line280">280: </a>    close(fd);
<a name="line281">281: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</a>(pathname, PETSC_MAX_PATH_LEN, MPI_CHAR, 0, ctrl-&gt;omp_comm);
<a name="line282">282: </a>    <font color="#B22222">/* this <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a> is to wait slaves to open the file before master unlinks it */</font>
<a name="line283">283: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a>(ctrl-&gt;omp_comm);
<a name="line284">284: </a>    unlink(pathname);
<a name="line285">285: </a>  } <font color="#4169E1">else</font> {
<a name="line286">286: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</a>(pathname, PETSC_MAX_PATH_LEN, MPI_CHAR, 0, ctrl-&gt;omp_comm);
<a name="line287">287: </a>    fd = open(pathname, O_RDWR);
<a name="line289">289: </a>    baseptr = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
<a name="line291">291: </a>    close(fd);
<a name="line292">292: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a>(ctrl-&gt;omp_comm);
<a name="line293">293: </a>  }
<a name="line294">294: </a><font color="#A020F0">  #else</font>
<a name="line295">295: </a>  size = ctrl-&gt;is_omp_master ? <font color="#4169E1">sizeof</font>(pthread_barrier_t) : 0;
<a name="line296">296: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Win_allocate_shared.html#MPI_Win_allocate_shared">MPI_Win_allocate_shared</a>(size, 1, MPI_INFO_NULL, ctrl-&gt;omp_comm, &amp;baseptr, &amp;ctrl-&gt;omp_win);
<a name="line297">297: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Win_shared_query.html#MPI_Win_shared_query">MPI_Win_shared_query</a>(ctrl-&gt;omp_win, 0, &amp;size, &amp;disp_unit, &amp;baseptr);
<a name="line298">298: </a><font color="#A020F0">  #endif</font>
<a name="line299">299: </a>  ctrl-&gt;barrier = (pthread_barrier_t *)baseptr;

<a name="line301">301: </a>  <font color="#B22222">/* omp master initializes the barrier */</font>
<a name="line302">302: </a>  <font color="#4169E1">if</font> (ctrl-&gt;is_omp_master) {
<a name="line303">303: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_size.html#MPI_Comm_size">MPI_Comm_size</a>(ctrl-&gt;omp_comm, &amp;ctrl-&gt;omp_comm_size);
<a name="line304">304: </a>    pthread_barrierattr_init(&amp;attr);
<a name="line305">305: </a>    pthread_barrierattr_setpshared(&amp;attr, PTHREAD_PROCESS_SHARED); <font color="#B22222">/* make the barrier also work for processes */</font>
<a name="line306">306: </a>    pthread_barrier_init(ctrl-&gt;barrier, &amp;attr, (unsigned int)ctrl-&gt;omp_comm_size);
<a name="line307">307: </a>    pthread_barrierattr_destroy(&amp;attr);
<a name="line308">308: </a>  }

<a name="line310">310: </a>  <font color="#B22222">/* this <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a> is to make sure the omp barrier is initialized before slaves use it */</font>
<a name="line311">311: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a>(ctrl-&gt;omp_comm);
<a name="line312">312: </a>  <font color="#4169E1">return</font> 0;
<a name="line313">313: </a>}

<a name="line315">315: </a><font color="#B22222">/* Destroy the pthread barrier in the PETSc OpenMP controller */</font>
<a name="line316">316: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlDestroyBarrier"></a>static inline <a href="../../../docs/manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> PetscOmpCtrlDestroyBarrier(PetscOmpCtrl ctrl)</font></strong>
<a name="line317">317: </a>{
<a name="line318">318: </a>  <font color="#B22222">/* this <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a> is to make sure slaves have finished using the omp barrier before master destroys it */</font>
<a name="line319">319: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a>(ctrl-&gt;omp_comm);
<a name="line320">320: </a>  <font color="#4169E1">if</font> (ctrl-&gt;is_omp_master) pthread_barrier_destroy(ctrl-&gt;barrier);

<a name="line322">322: </a><font color="#A020F0">  #if defined(USE_MMAP_ALLOCATE_SHARED_MEMORY) &amp;&amp; defined(PETSC_HAVE_MMAP)</font>
<a name="line323">323: </a>  munmap(ctrl-&gt;barrier, <font color="#4169E1">sizeof</font>(pthread_barrier_t));
<a name="line324">324: </a><font color="#A020F0">  #else</font>
<a name="line325">325: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Win_free.html#MPI_Win_free">MPI_Win_free</a>(&amp;ctrl-&gt;omp_win);
<a name="line326">326: </a><font color="#A020F0">  #endif</font>
<a name="line327">327: </a>  <font color="#4169E1">return</font> 0;
<a name="line328">328: </a>}

<a name="line330">330: </a><font color="#B22222">/*@C</font>
<a name="line331">331: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscOmpCtrlCreate.html">PetscOmpCtrlCreate</a> - create a PETSc OpenMP controller, which manages PETSc's interaction with third party libraries that use OpenMP</font>

<a name="line333">333: </a><font color="#B22222">    Input Parameters:</font>
<a name="line334">334: </a><font color="#B22222">+   petsc_comm - a communicator some PETSc object (for example, a matrix) lives in</font>
<a name="line335">335: </a><font color="#B22222">-   nthreads   - number of threads per MPI rank to spawn in a library using OpenMP. If nthreads = -1, let PETSc decide a suitable value</font>

<a name="line337">337: </a><font color="#B22222">    Output Parameter:</font>
<a name="line338">338: </a><font color="#B22222">.   pctrl      - a PETSc OpenMP controller</font>

<a name="line340">340: </a><font color="#B22222">    Level: developer</font>

<a name="line342">342: </a><font color="#B22222">    Developer Note:</font>
<a name="line343">343: </a><font color="#B22222">    Possibly use the variable `PetscNumOMPThreads` to determine the number for threads to use</font>

<a name="line345">345: </a><font color="#B22222">.seealso: `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlDestroy.html">PetscOmpCtrlDestroy</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlGetOmpComms.html">PetscOmpCtrlGetOmpComms</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlBarrier.html">PetscOmpCtrlBarrier</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterBegin.html">PetscOmpCtrlOmpRegionOnMasterBegin</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterEnd.html">PetscOmpCtrlOmpRegionOnMasterEnd</a>()`,</font>
<a name="line346">346: </a><font color="#B22222">@*/</font>
<a name="line347">347: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlCreate"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscOmpCtrlCreate.html">PetscOmpCtrlCreate</a>(<a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a> petsc_comm, <a href="../../../docs/manualpages/Sys/PetscInt.html">PetscInt</a> nthreads, PetscOmpCtrl *pctrl)</font></strong>
<a name="line348">348: </a>{
<a name="line349">349: </a>  PetscOmpCtrl   ctrl;
<a name="line350">350: </a>  unsigned long *cpu_ulongs = NULL;
<a name="line351">351: </a>  <a href="../../../docs/manualpages/Sys/PetscInt.html">PetscInt</a>       i, nr_cpu_ulongs;
<a name="line352">352: </a>  PetscShmComm   pshmcomm;
<a name="line353">353: </a>  <a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a>       shm_comm;
<a name="line354">354: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html">PetscMPIInt</a>    shm_rank, shm_comm_size, omp_rank, color;
<a name="line355">355: </a>  <a href="../../../docs/manualpages/Sys/PetscInt.html">PetscInt</a>       num_packages, num_cores;

<a name="line357">357: </a>  <a href="../../../docs/manualpages/Sys/PetscNew.html">PetscNew</a>(&amp;ctrl);

<a name="line359">359: </a>  <font color="#B22222">/*=================================================================================</font>
<a name="line360">360: </a><font color="#B22222">    Init hwloc</font>
<a name="line361">361: </a><font color="#B22222">   ==================================================================================*/</font>
<a name="line362">362: </a>  hwloc_topology_init(&amp;ctrl-&gt;topology);
<a name="line363">363: </a><font color="#A020F0">  #if HWLOC_API_VERSION &gt;= 0x00020000</font>
<a name="line364">364: </a>  <font color="#B22222">/* to filter out unneeded info and have faster hwloc_topology_load */</font>
<a name="line365">365: </a>  hwloc_topology_set_all_types_filter(ctrl-&gt;topology, HWLOC_TYPE_FILTER_KEEP_NONE);
<a name="line366">366: </a>  hwloc_topology_set_type_filter(ctrl-&gt;topology, HWLOC_OBJ_CORE, HWLOC_TYPE_FILTER_KEEP_ALL);
<a name="line367">367: </a><font color="#A020F0">  #endif</font>
<a name="line368">368: </a>  hwloc_topology_load(ctrl-&gt;topology);

<a name="line370">370: </a>  <font color="#B22222">/*=================================================================================</font>
<a name="line371">371: </a><font color="#B22222">    Split petsc_comm into multiple omp_comms. Ranks in an omp_comm have access to</font>
<a name="line372">372: </a><font color="#B22222">    physically shared memory. Rank 0 of each omp_comm is called an OMP master, and</font>
<a name="line373">373: </a><font color="#B22222">    others are called slaves. OMP Masters make up a new comm called omp_master_comm,</font>
<a name="line374">374: </a><font color="#B22222">    which is usually passed to third party libraries.</font>
<a name="line375">375: </a><font color="#B22222">   ==================================================================================*/</font>

<a name="line377">377: </a>  <font color="#B22222">/* fetch the stored shared memory communicator */</font>
<a name="line378">378: </a>  <a href="../../../docs/manualpages/Sys/PetscShmCommGet.html">PetscShmCommGet</a>(petsc_comm, &amp;pshmcomm);
<a name="line379">379: </a>  <a href="../../../docs/manualpages/Sys/PetscShmCommGetMpiShmComm.html">PetscShmCommGetMpiShmComm</a>(pshmcomm, &amp;shm_comm);

<a name="line381">381: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_rank.html#MPI_Comm_rank">MPI_Comm_rank</a>(shm_comm, &amp;shm_rank);
<a name="line382">382: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_size.html#MPI_Comm_size">MPI_Comm_size</a>(shm_comm, &amp;shm_comm_size);

<a name="line384">384: </a>  <font color="#B22222">/* PETSc decides nthreads, which is the smaller of shm_comm_size or cores per package(socket) */</font>
<a name="line385">385: </a>  <font color="#4169E1">if</font> (nthreads == -1) {
<a name="line386">386: </a>    num_packages = hwloc_get_nbobjs_by_type(ctrl-&gt;topology, HWLOC_OBJ_PACKAGE) &lt;= 0 ? 1 : hwloc_get_nbobjs_by_type(ctrl-&gt;topology, HWLOC_OBJ_PACKAGE);
<a name="line387">387: </a>    num_cores    = hwloc_get_nbobjs_by_type(ctrl-&gt;topology, HWLOC_OBJ_CORE) &lt;= 0 ? 1 : hwloc_get_nbobjs_by_type(ctrl-&gt;topology, HWLOC_OBJ_CORE);
<a name="line388">388: </a>    nthreads     = num_cores / num_packages;
<a name="line389">389: </a>    <font color="#4169E1">if</font> (nthreads &gt; shm_comm_size) nthreads = shm_comm_size;
<a name="line390">390: </a>  }

<a name="line393">393: </a>  <font color="#4169E1">if</font> (shm_comm_size % nthreads) <a href="../../../docs/manualpages/Sys/PetscPrintf.html">PetscPrintf</a>(petsc_comm, <font color="#666666">"Warning: number of OpenMP threads %"</font> PetscInt_FMT <font color="#666666">" is not a factor of the MPI shared memory communicator size %d, which may cause load-imbalance!\n"</font>, nthreads, shm_comm_size);

<a name="line395">395: </a>  <font color="#B22222">/* split shm_comm into a set of omp_comms with each of size nthreads. Ex., if</font>
<a name="line396">396: </a><font color="#B22222">     shm_comm_size=16, nthreads=8, then ranks 0~7 get color 0 and ranks 8~15 get</font>
<a name="line397">397: </a><font color="#B22222">     color 1. They are put in two omp_comms. Note that petsc_ranks may or may not</font>
<a name="line398">398: </a><font color="#B22222">     be consecutive in a shm_comm, but shm_ranks always run from 0 to shm_comm_size-1.</font>
<a name="line399">399: </a><font color="#B22222">     Use 0 as key so that rank ordering wont change in new comm.</font>
<a name="line400">400: </a><font color="#B22222">   */</font>
<a name="line401">401: </a>  color = shm_rank / nthreads;
<a name="line402">402: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_split.html#MPI_Comm_split">MPI_Comm_split</a>(shm_comm, color, 0 <font color="#B22222">/*key*/</font>, &amp;ctrl-&gt;omp_comm);

<a name="line404">404: </a>  <font color="#B22222">/* put rank 0's in omp_comms (i.e., master ranks) into a new comm - omp_master_comm */</font>
<a name="line405">405: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_rank.html#MPI_Comm_rank">MPI_Comm_rank</a>(ctrl-&gt;omp_comm, &amp;omp_rank);
<a name="line406">406: </a>  <font color="#4169E1">if</font> (!omp_rank) {
<a name="line407">407: </a>    ctrl-&gt;is_omp_master = <a href="../../../docs/manualpages/Sys/PETSC_TRUE.html">PETSC_TRUE</a>; <font color="#B22222">/* master */</font>
<a name="line408">408: </a>    color               = 0;
<a name="line409">409: </a>  } <font color="#4169E1">else</font> {
<a name="line410">410: </a>    ctrl-&gt;is_omp_master = <a href="../../../docs/manualpages/Sys/PETSC_FALSE.html">PETSC_FALSE</a>;   <font color="#B22222">/* slave */</font>
<a name="line411">411: </a>    color               = MPI_UNDEFINED; <font color="#B22222">/* to make slaves get omp_master_comm = MPI_COMM_NULL in <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_split.html#MPI_Comm_split">MPI_Comm_split</a> */</font>
<a name="line412">412: </a>  }
<a name="line413">413: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_split.html#MPI_Comm_split">MPI_Comm_split</a>(petsc_comm, color, 0 <font color="#B22222">/*key*/</font>, &amp;ctrl-&gt;omp_master_comm);

<a name="line415">415: </a>  <font color="#B22222">/*=================================================================================</font>
<a name="line416">416: </a><font color="#B22222">    Each omp_comm has a pthread_barrier_t in its shared memory, which is used to put</font>
<a name="line417">417: </a><font color="#B22222">    slave ranks in sleep and idle their CPU, so that the master can fork OMP threads</font>
<a name="line418">418: </a><font color="#B22222">    and run them on the idle CPUs.</font>
<a name="line419">419: </a><font color="#B22222">   ==================================================================================*/</font>
<a name="line420">420: </a>  PetscOmpCtrlCreateBarrier(ctrl);

<a name="line422">422: </a>  <font color="#B22222">/*=================================================================================</font>
<a name="line423">423: </a><font color="#B22222">    omp master logs its cpu binding (i.e., cpu set) and computes a new binding that</font>
<a name="line424">424: </a><font color="#B22222">    is the union of the bindings of all ranks in the omp_comm</font>
<a name="line425">425: </a><font color="#B22222">    =================================================================================*/</font>

<a name="line427">427: </a>  ctrl-&gt;cpuset = hwloc_bitmap_alloc();
<a name="line429">429: </a>  hwloc_get_cpubind(ctrl-&gt;topology, ctrl-&gt;cpuset, HWLOC_CPUBIND_PROCESS);

<a name="line431">431: </a>  <font color="#B22222">/* hwloc main developer said they will add new APIs hwloc_bitmap_{nr,to,from}_ulongs in 2.1 to help us simplify the following bitmap pack/unpack code */</font>
<a name="line432">432: </a>  nr_cpu_ulongs = (hwloc_bitmap_last(hwloc_topology_get_topology_cpuset(ctrl-&gt;topology)) + <font color="#4169E1">sizeof</font>(unsigned long) * 8) / <font color="#4169E1">sizeof</font>(unsigned long) / 8;
<a name="line433">433: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html">PetscMalloc1</a>(nr_cpu_ulongs, &amp;cpu_ulongs);
<a name="line434">434: </a>  <font color="#4169E1">if</font> (nr_cpu_ulongs == 1) {
<a name="line435">435: </a>    cpu_ulongs[0] = hwloc_bitmap_to_ulong(ctrl-&gt;cpuset);
<a name="line436">436: </a>  } <font color="#4169E1">else</font> {
<a name="line437">437: </a>    <font color="#4169E1">for</font> (i = 0; i &lt; nr_cpu_ulongs; i++) cpu_ulongs[i] = hwloc_bitmap_to_ith_ulong(ctrl-&gt;cpuset, (unsigned)i);
<a name="line438">438: </a>  }

<a name="line440">440: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Reduce.html#MPI_Reduce">MPI_Reduce</a>(ctrl-&gt;is_omp_master ? MPI_IN_PLACE : cpu_ulongs, cpu_ulongs, nr_cpu_ulongs, MPI_UNSIGNED_LONG, MPI_BOR, 0, ctrl-&gt;omp_comm);

<a name="line442">442: </a>  <font color="#4169E1">if</font> (ctrl-&gt;is_omp_master) {
<a name="line443">443: </a>    ctrl-&gt;omp_cpuset = hwloc_bitmap_alloc();
<a name="line445">445: </a>    <font color="#4169E1">if</font> (nr_cpu_ulongs == 1) {
<a name="line446">446: </a><font color="#A020F0">  #if HWLOC_API_VERSION &gt;= 0x00020000</font>
<a name="line447">447: </a>      hwloc_bitmap_from_ulong(ctrl-&gt;omp_cpuset, cpu_ulongs[0]);
<a name="line448">448: </a><font color="#A020F0">  #else</font>
<a name="line449">449: </a>      hwloc_bitmap_from_ulong(ctrl-&gt;omp_cpuset, cpu_ulongs[0]);
<a name="line450">450: </a><font color="#A020F0">  #endif</font>
<a name="line451">451: </a>    } <font color="#4169E1">else</font> {
<a name="line452">452: </a>      <font color="#4169E1">for</font> (i = 0; i &lt; nr_cpu_ulongs; i++) {
<a name="line453">453: </a><font color="#A020F0">  #if HWLOC_API_VERSION &gt;= 0x00020000</font>
<a name="line454">454: </a>        hwloc_bitmap_set_ith_ulong(ctrl-&gt;omp_cpuset, (unsigned)i, cpu_ulongs[i]);
<a name="line455">455: </a><font color="#A020F0">  #else</font>
<a name="line456">456: </a>        hwloc_bitmap_set_ith_ulong(ctrl-&gt;omp_cpuset, (unsigned)i, cpu_ulongs[i]);
<a name="line457">457: </a><font color="#A020F0">  #endif</font>
<a name="line458">458: </a>      }
<a name="line459">459: </a>    }
<a name="line460">460: </a>  }
<a name="line461">461: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html">PetscFree</a>(cpu_ulongs);
<a name="line462">462: </a>  *pctrl = ctrl;
<a name="line463">463: </a>  <font color="#4169E1">return</font> 0;
<a name="line464">464: </a>}

<a name="line466">466: </a><font color="#B22222">/*@C</font>
<a name="line467">467: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscOmpCtrlDestroy.html">PetscOmpCtrlDestroy</a> - destroy the PETSc OpenMP controller</font>

<a name="line469">469: </a><font color="#B22222">    Input Parameter:</font>
<a name="line470">470: </a><font color="#B22222">.   pctrl  - a PETSc OpenMP controller</font>

<a name="line472">472: </a><font color="#B22222">    Level: developer</font>

<a name="line474">474: </a><font color="#B22222">.seealso: `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlCreate.html">PetscOmpCtrlCreate</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlGetOmpComms.html">PetscOmpCtrlGetOmpComms</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlBarrier.html">PetscOmpCtrlBarrier</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterBegin.html">PetscOmpCtrlOmpRegionOnMasterBegin</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterEnd.html">PetscOmpCtrlOmpRegionOnMasterEnd</a>()`,</font>
<a name="line475">475: </a><font color="#B22222">@*/</font>
<a name="line476">476: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlDestroy"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscOmpCtrlDestroy.html">PetscOmpCtrlDestroy</a>(PetscOmpCtrl *pctrl)</font></strong>
<a name="line477">477: </a>{
<a name="line478">478: </a>  PetscOmpCtrl ctrl = *pctrl;

<a name="line480">480: </a>  hwloc_bitmap_free(ctrl-&gt;cpuset);
<a name="line481">481: </a>  hwloc_topology_destroy(ctrl-&gt;topology);
<a name="line482">482: </a>  PetscOmpCtrlDestroyBarrier(ctrl);
<a name="line483">483: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_free.html#MPI_Comm_free">MPI_Comm_free</a>(&amp;ctrl-&gt;omp_comm);
<a name="line484">484: </a>  <font color="#4169E1">if</font> (ctrl-&gt;is_omp_master) {
<a name="line485">485: </a>    hwloc_bitmap_free(ctrl-&gt;omp_cpuset);
<a name="line486">486: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_free.html#MPI_Comm_free">MPI_Comm_free</a>(&amp;ctrl-&gt;omp_master_comm);
<a name="line487">487: </a>  }
<a name="line488">488: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html">PetscFree</a>(ctrl);
<a name="line489">489: </a>  <font color="#4169E1">return</font> 0;
<a name="line490">490: </a>}

<a name="line492">492: </a><font color="#B22222">/*@C</font>
<a name="line493">493: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscOmpCtrlGetOmpComms.html">PetscOmpCtrlGetOmpComms</a> - Get MPI communicators from a PETSc OMP controller</font>

<a name="line495">495: </a><font color="#B22222">    Input Parameter:</font>
<a name="line496">496: </a><font color="#B22222">.   ctrl - a PETSc OMP controller</font>

<a name="line498">498: </a><font color="#B22222">    Output Parameters:</font>
<a name="line499">499: </a><font color="#B22222">+   omp_comm         - a communicator that includes a master rank and slave ranks where master spawns threads</font>
<a name="line500">500: </a><font color="#B22222">.   omp_master_comm  - on master ranks, return a communicator that include master ranks of each omp_comm;</font>
<a name="line501">501: </a><font color="#B22222">                       on slave ranks, `MPI_COMM_NULL` will be return in reality.</font>
<a name="line502">502: </a><font color="#B22222">-   is_omp_master    - true if the calling process is an OMP master rank.</font>

<a name="line504">504: </a><font color="#B22222">    Note:</font>
<a name="line505">505: </a><font color="#B22222">    Any output parameter can be NULL. The parameter is just ignored.</font>

<a name="line507">507: </a><font color="#B22222">    Level: developer</font>

<a name="line509">509: </a><font color="#B22222">.seealso: `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlCreate.html">PetscOmpCtrlCreate</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlDestroy.html">PetscOmpCtrlDestroy</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlBarrier.html">PetscOmpCtrlBarrier</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterBegin.html">PetscOmpCtrlOmpRegionOnMasterBegin</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterEnd.html">PetscOmpCtrlOmpRegionOnMasterEnd</a>()`,</font>
<a name="line510">510: </a><font color="#B22222">@*/</font>
<a name="line511">511: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlGetOmpComms"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscOmpCtrlGetOmpComms.html">PetscOmpCtrlGetOmpComms</a>(PetscOmpCtrl ctrl, <a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a> *omp_comm, <a href="../../../docs/manualpages/Sys/MPI_Comm.html">MPI_Comm</a> *omp_master_comm, <a href="../../../docs/manualpages/Sys/PetscBool.html">PetscBool</a> *is_omp_master)</font></strong>
<a name="line512">512: </a>{
<a name="line513">513: </a>  <font color="#4169E1">if</font> (omp_comm) *omp_comm = ctrl-&gt;omp_comm;
<a name="line514">514: </a>  <font color="#4169E1">if</font> (omp_master_comm) *omp_master_comm = ctrl-&gt;omp_master_comm;
<a name="line515">515: </a>  <font color="#4169E1">if</font> (is_omp_master) *is_omp_master = ctrl-&gt;is_omp_master;
<a name="line516">516: </a>  <font color="#4169E1">return</font> 0;
<a name="line517">517: </a>}

<a name="line519">519: </a><font color="#B22222">/*@C</font>
<a name="line520">520: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscOmpCtrlBarrier.html">PetscOmpCtrlBarrier</a> - Do barrier on MPI ranks in omp_comm contained by the PETSc OMP controller (to let slave ranks free their CPU)</font>

<a name="line522">522: </a><font color="#B22222">    Input Parameter:</font>
<a name="line523">523: </a><font color="#B22222">.   ctrl - a PETSc OMP controller</font>

<a name="line525">525: </a><font color="#B22222">    Notes:</font>
<a name="line526">526: </a><font color="#B22222">    this is a pthread barrier on MPI ranks. Using `<a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a>()` instead is conceptually correct. But MPI standard does not</font>
<a name="line527">527: </a><font color="#B22222">    require processes blocked by `<a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a>()` free their CPUs to let other processes progress. In practice, to minilize latency,</font>
<a name="line528">528: </a><font color="#B22222">    MPI ranks stuck in `<a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a>()` keep polling and do not free CPUs. In contrast, pthread_barrier has this requirement.</font>

<a name="line530">530: </a><font color="#B22222">    A code using `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlBarrier.html">PetscOmpCtrlBarrier</a>()` would be like this,</font>
<a name="line531">531: </a><font color="#B22222">.vb</font>
<a name="line532">532: </a><font color="#B22222">    if (is_omp_master) {</font>
<a name="line533">533: </a><font color="#B22222">      <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterBegin.html">PetscOmpCtrlOmpRegionOnMasterBegin</a>(ctrl);</font>
<a name="line534">534: </a><font color="#B22222">      Call the library using OpenMP</font>
<a name="line535">535: </a><font color="#B22222">      <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterEnd.html">PetscOmpCtrlOmpRegionOnMasterEnd</a>(ctrl);</font>
<a name="line536">536: </a><font color="#B22222">    }</font>
<a name="line537">537: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscOmpCtrlBarrier.html">PetscOmpCtrlBarrier</a>(ctrl);</font>
<a name="line538">538: </a><font color="#B22222">.ve</font>

<a name="line540">540: </a><font color="#B22222">    Level: developer</font>

<a name="line542">542: </a><font color="#B22222">.seealso: `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterBegin.html">PetscOmpCtrlOmpRegionOnMasterBegin</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterEnd.html">PetscOmpCtrlOmpRegionOnMasterEnd</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlCreate.html">PetscOmpCtrlCreate</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlDestroy.html">PetscOmpCtrlDestroy</a>()`,</font>
<a name="line543">543: </a><font color="#B22222">@*/</font>
<a name="line544">544: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlBarrier"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscOmpCtrlBarrier.html">PetscOmpCtrlBarrier</a>(PetscOmpCtrl ctrl)</font></strong>
<a name="line545">545: </a>{
<a name="line546">546: </a>  int err;

<a name="line548">548: </a>  err = pthread_barrier_wait(ctrl-&gt;barrier);
<a name="line550">550: </a>  <font color="#4169E1">return</font> 0;
<a name="line551">551: </a>}

<a name="line553">553: </a><font color="#B22222">/*@C</font>
<a name="line554">554: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterBegin.html">PetscOmpCtrlOmpRegionOnMasterBegin</a> - Mark the beginning of an OpenMP library call on master ranks</font>

<a name="line556">556: </a><font color="#B22222">    Input Parameter:</font>
<a name="line557">557: </a><font color="#B22222">.   ctrl - a PETSc OMP controller</font>

<a name="line559">559: </a><font color="#B22222">    Note:</font>
<a name="line560">560: </a><font color="#B22222">    Only master ranks can call this function. Call `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlGetOmpComms.html">PetscOmpCtrlGetOmpComms</a>()` to know if this is a master rank.</font>
<a name="line561">561: </a><font color="#B22222">    This function changes CPU binding of master ranks and nthreads-var of OpenMP runtime</font>

<a name="line563">563: </a><font color="#B22222">    Level: developer</font>

<a name="line565">565: </a><font color="#B22222">.seealso: `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterEnd.html">PetscOmpCtrlOmpRegionOnMasterEnd</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlCreate.html">PetscOmpCtrlCreate</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlDestroy.html">PetscOmpCtrlDestroy</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlBarrier.html">PetscOmpCtrlBarrier</a>()`</font>
<a name="line566">566: </a><font color="#B22222">@*/</font>
<a name="line567">567: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlOmpRegionOnMasterBegin"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterBegin.html">PetscOmpCtrlOmpRegionOnMasterBegin</a>(PetscOmpCtrl ctrl)</font></strong>
<a name="line568">568: </a>{
<a name="line569">569: </a>  hwloc_set_cpubind(ctrl-&gt;topology, ctrl-&gt;omp_cpuset, HWLOC_CPUBIND_PROCESS);
<a name="line570">570: </a>  omp_set_num_threads(ctrl-&gt;omp_comm_size); <font color="#B22222">/* may override the OMP_NUM_THREAD env var */</font>
<a name="line571">571: </a>  <font color="#4169E1">return</font> 0;
<a name="line572">572: </a>}

<a name="line574">574: </a><font color="#B22222">/*@C</font>
<a name="line575">575: </a><font color="#B22222">   <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterEnd.html">PetscOmpCtrlOmpRegionOnMasterEnd</a> - Mark the end of an OpenMP library call on master ranks</font>

<a name="line577">577: </a><font color="#B22222">   Input Parameter:</font>
<a name="line578">578: </a><font color="#B22222">.  ctrl - a PETSc OMP controller</font>

<a name="line580">580: </a><font color="#B22222">   Note:</font>
<a name="line581">581: </a><font color="#B22222">   Only master ranks can call this function. Call `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlGetOmpComms.html">PetscOmpCtrlGetOmpComms</a>()` to know if this is a master rank.</font>
<a name="line582">582: </a><font color="#B22222">   This function restores the CPU binding of master ranks and set and nthreads-var of OpenMP runtime to 1.</font>

<a name="line584">584: </a><font color="#B22222">   Level: developer</font>

<a name="line586">586: </a><font color="#B22222">.seealso: `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterBegin.html">PetscOmpCtrlOmpRegionOnMasterBegin</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlCreate.html">PetscOmpCtrlCreate</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlDestroy.html">PetscOmpCtrlDestroy</a>()`, `<a href="../../../docs/manualpages/Sys/PetscOmpCtrlBarrier.html">PetscOmpCtrlBarrier</a>()`</font>
<a name="line587">587: </a><font color="#B22222">@*/</font>
<a name="line588">588: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlOmpRegionOnMasterEnd"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterEnd.html">PetscOmpCtrlOmpRegionOnMasterEnd</a>(PetscOmpCtrl ctrl)</font></strong>
<a name="line589">589: </a>{
<a name="line590">590: </a>  hwloc_set_cpubind(ctrl-&gt;topology, ctrl-&gt;cpuset, HWLOC_CPUBIND_PROCESS);
<a name="line591">591: </a>  omp_set_num_threads(1);
<a name="line592">592: </a>  <font color="#4169E1">return</font> 0;
<a name="line593">593: </a>}

<a name="line595">595: </a><strong><font color="#228B22">  #undef USE_MMAP_ALLOCATE_SHARED_MEMORY</font></strong>
<a name="line596">596: </a><font color="#A020F0">#endif </font><font color="#B22222">/* defined(PETSC_HAVE_OPENMP_SUPPORT) */</font><font color="#A020F0"></font>
</pre>
</body>

</html>
